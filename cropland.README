## Workflow:
1. Find dataset in AI Cropland S3 data bucket

### If using 64x64 tiles and dataset has tiled64 folder with zip file(s),
2. download the tiled64 folder with (from the root project directory)
 -aws s3 cp "s3://path/to/dataset/tiled64/" path/to/target/dir/ --recursive
3. unzip contents and skip to step 12

Note: Tiling data to 64x64 is very time consuming, hence the zip file

### If the dataset folder contains no tiled64 folder
1. Download tif files in that dataset's folder with (from the root project directory)
 -aws s3 cp "s3://path/to/dataset/" ./prep_tiles --recursive

The next step is to pad out the tif files so they're the same dimensions as their CDL masks. We do this by filling in all transparents values as 0
2. Copy raster_padder.py to the downloaded dataset directory
3. create a folder in the downloaded dataset directory it called "padded"
4. run python3 raster_padder.py from the downloaded dataset directory 
5. move the newly created tif files of padded dir along with CDL_[dataset name]_mask.tif to prep_tiles

The following steps take our now properly aligned and padded data and ground truth mask and breaks it into smaller tif images
6. Create two empty directories called "tiles" and "mask_tiles" in the prep_tiles directory
7. Move prep_util.sh from the scripts/ to the root project directory
8. Run the script with  -./prep_util.sh [tile size (default is 256)]
9. Wait for prep_utils to finish (Depending on tile size, this *will* take a while)

Now that we have our tiled data we check each sample's pixel class distribution (in the case of a binary mask, 0 for non-crop pixel and 1 for crop pixel)
The following script checks each mask tile, and removes the corresponding sample data if the mask tile's
non-crop pixels are over 80% of the image (to change this threshold, edit the valid mask function).
10. Once prep_utils is finished, run - python3 sample_selector.py
11. Enter the dataset's State acronym (ie: AK, WA, OR)
12. Enter the year of the dataset (2019, 2018, etc)
13. If data is only meant for the model to predict with, enter 'test', otherwise enter 'train'
14. Wait for sample_selector.py to finish (This will *also* take a while depending on your tile size)

There should now be a directory for the dataset in the project root directory (ie: WA_2018, OR_2019, etc)
In it should be a json metadata file, and two directories with subfolders with the same name as the created dataset directory
15. Move all images in prep_tiles/mask_tiles to a new directory in this dataset's directory called "[name_of_dataset]_masks" (IE: WA_2018_masks, OR_2019_masks)
16. (Optional) Depending on how long that took, you might want to archive the dataset and its masks as a zip or tar. (I suggest using tar with pigz on the folder for speed)

Repeat steps 1-16 for each dataset you wish to train with

Create the dataset directory to store all the created subdatasets in
17. Make a directory in AI_Water/datasets/ for the region/dataset you wish to train with (ex: US_NW, US_SW, US_MISC, etc)
18. Make three directories in the new dataset directory called "test", "train", and "masks"
19. From each subdataset you have in the project root directory, copy the subfolders in each subdataset's test and train folders
    into the main dataset's test or train folders, and move the subdataset's mask directory to the dataset's masks directory

## Your datasets directory should look something like this

datasets/
    |--US_NW/
       |--WA_2018_metadata.json
       |--OR_2019_metadata.json
       |--test/
       |   |--WA_2018/
       |   |    |-- S1....VV_ulx_0_uly_0.tif
       |   |    |-- S1....VH_ulx_0_uly_0.tif
       |   |--OR_2019/
       |
       |--train/
       |   |--WA_2018/
       |   |    |-- S1....VV_ulx_64_uly_512.tif
       |   |    |-- S1....VH_ulx_64_uly_512.tif
       |   |--OR_2019/
       |
       |--masks/
           |--WA_2018_masks/
           |    |-- CDL_WA_2018_mask_ulx_64_uly_512.tif
           |--OR_2019_masks/
                |-- CDL_OR_2019_mask_ulx_0_uly_128.tif

20. If all steps were performed properly, enter the following line to begin training
- python3 main.py train name_of_new_model [name_of_regional_dataset]

ex:
- python3 main.py train cool_new_model US_NW